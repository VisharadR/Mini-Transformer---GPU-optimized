# Mini Transformer ‚Äì GPU Optimized


This project is about building a **tiny version of a Transformer model** (the same kind of model used in ChatGPT, BERT, and GPT models) ‚Äî but in a way that‚Äôs **fast, lightweight, and optimized for GPUs**.  

It also shows how to make the model **portable** so it can run not just in PyTorch (where it is trained), but also in **ONNX** (for faster deployment) and **CoreML** (for Apple devices like iPhones and Macs).  

Think of this project as a **mini-lab for AI model optimization**:  
- Train and test the model üèãÔ∏è  
- Convert it into formats for different platforms üîÑ  
- Make it smaller and faster ‚ö°  
- Verify that it works correctly ‚úÖ  

---

## üåü Why This Project Matters  

Modern AI models are **big** and **slow**. They work great on powerful servers but are hard to deploy on smaller devices. This project shows:  

- How to **shrink models** without losing accuracy.  
- How to **move models** between platforms (PyTorch ‚Üí ONNX ‚Üí CoreML).  
- How to **make them faster** for real-world applications like mobile apps, websites, or edge devices.  

In short: *learn how AI goes from the lab to your phone.* üì±  

---

## ‚ú® Features

- **Custom CUDA Kernels**: Implemented FlashAttention-style kernels for efficient attention on long sequences.
- **Fused Ops**: Combined residual + layer normalization into a single GPU kernel for lower overhead.
- **Export Pipelines**:
  - PyTorch ‚Üí ONNX ‚Üí CoreML
  - Supports CPU, GPU, and Apple Silicon deployment
- **Benchmark Tools**:
  - Compare PyTorch vs. ONNX vs. CoreML performance
  - p50 / p95 latency profiling
- **Mini Runtime (vLLM-style)**:
  - Dynamic batching of multiple prompts
  - Streaming token callbacks
  - Torch backend (ONNX optional)
  - End-to-end demo for interactive text generation

---

## üß© How It Works

- **Model Optimizations**:
  - Attention kernels avoid quadratic cost on long sequences.
  -Fused LayerNorm+Residual reduces memory reads/writes.

- **Export Pipelines**:
  - Convert PyTorch model ‚Üí ONNX ‚Üí CoreML.
  -Verify parity at the logits level to ensure correctness.

- **Runtime**:
  - Accepts multiple prompts.
  - Splits into prefill (process full prompt) and decode (one token at a time).
  - Uses a simple scheduler for dynamic batching.
  - Streams outputs back with callbacks.

---

## üì¶ Installation

Clone the repository and create a virtual environment:
```bash
git clone https://github.com/VisharadR/Mini-Transformer---GPU-optimized.git
cd mini-transformer-gpu-optimized
```
```bash
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate

pip install -r requirements.txt 
```
## ‚ö° Quickstart
1. Run Baseline Benchmark

Benchmark the PyTorch model performance:
```bash
python baselines/benchmark.py
```

Example output:
```bash
=== Benchmarking TinyTransformerPostLN ===
seq=128, batch=1, p50=4.00ms, p95=4.01ms, tokens/s=35356.6
```

## üîÑ Export Models

Export to ONNX
```bash
python scripts/export_onnx.py
```

This produces models/model.onnx.

Verify ONNX Parity
```bash
python scripts/verify_onnx.py
```

Output should show very small differences (max|Œîprobs| ‚â≤ 1e-4):
```bash
'great movie!': max|Œîlogits|=0.00000, max|Œîprobs|=0.00000, py=1, onnx=1
```
Benchmark ONNX Runtime
```bash
python scripts/bench_onnx.py
```

Run the Runtime Demo
- Try interactive generation with the custom runtime:
```bash
python scripts/run_runtime_demo.py --prompts "hello gpu" "deep learning" --max_new 12
```

## üçè Export to CoreML (Mac only)

We provide two workflows:

A. Windows/Linux users: Skip CoreML export (not supported).

B. macOS users with Apple Silicon
Install CoreML tools:
```bash
pip install coremltools==7.2 numpy==1.26.4  # version compatible with numpy<2.0
python scripts/export_coreml.py
```

## ‚ö†Ô∏è Note: Torch 2.5.x is not fully tested with CoreML. For best results use Torch 2.2.x on macOS.


üîß Quantization

We support ONNX quantization (dynamic).

Run:
```bash
python scripts/quantize_onnx.py
```

This generates models/model.int8.onnx.
To verify:
```bash
python scripts/verify_onnx.py --model models/model.int8.onnx
```
## üß™ Continuous Integration (CI)

CI is set up to:

- Export the ONNX model.

- Run parity checks (PyTorch ‚Üî ONNX).

- Upload ONNX artifacts for download.

- No benchmarking is done in CI (benchmarks should be run locally).

## üìä Project Structure
```bash
baselines/           # PyTorch model + benchmark
  ‚îú‚îÄ‚îÄ benchmark_engine.py
  ‚îú‚îÄ‚îÄ benchmark_postln.py
  ‚îú‚îÄ‚îÄ benchmark.py
  ‚îú‚îÄ‚îÄ profile_engine.py
  ‚îú‚îÄ‚îÄ tiny_transformer_postln.py
  ‚îú‚îÄ‚îÄ tiny_transformer_torch.py
engine/
  ‚îú‚îÄ‚îÄ fused_attention.py
  ‚îú‚îÄ‚îÄ fused_lnresidual.py
  ‚îú‚îÄ‚îÄ perf_toggles.py
  ‚îú‚îÄ‚îÄ runner.py
engine_runtime/
  ‚îú‚îÄ‚îÄ api.py
  ‚îú‚îÄ‚îÄ engine_iface.py
  ‚îú‚îÄ‚îÄ engine_torch.py
  ‚îú‚îÄ‚îÄ kv_cache.py
  ‚îú‚îÄ‚îÄ sampling.py
  ‚îú‚îÄ‚îÄ scheduler.py
kernels/
  ‚îú‚îÄ‚îÄ attention_triton.py
models/              # Exported models (ONNX, CoreML, quantized)
  ‚îú‚îÄ‚îÄ intent_classifier.int8.onnx
  ‚îú‚îÄ‚îÄ intent_classifier.onnx
scripts/
  ‚îú‚îÄ‚îÄ export_onnx.py
  ‚îú‚îÄ‚îÄ verify_onnx.py
  ‚îú‚îÄ‚îÄ bench_onnx.py
  ‚îú‚îÄ‚îÄ quantize_onnx.py
  ‚îú‚îÄ‚îÄ export_coreml.py 
```
## ‚úÖ Roadmap

- Baseline PyTorch benchmark

- Export & verify ONNX

- ONNX Runtime benchmark

- ONNX quantization

- CoreML export (Mac only)

- Minimal CI parity check job

- Add more transformer variants (GPT-style, encoder-decoder)

## Run the Runtime

## üìå Notes

- Ensure NumPy < 2.0 for CoreML compatibility.

- CoreML export requires macOS + Apple Silicon.

- ONNX Runtime runs smoothly on CPU/GPU across platforms.
